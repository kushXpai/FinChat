{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfRtm324ngTW0L3Vb4Pj2I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kushXpai/FinChat/blob/main/FinChat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4Wh8ijKthrJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('/content/data.json', 'r') as f:\n",
        "    intents = json.load(f)\n",
        "\n",
        "print(intents)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMxFe3Mptv8r",
        "outputId": "f4be0d89-b80a-4650-f4d7-9a1c5ef318d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'intents': [{'tag': 'welcome', 'patterns': ['Hi', 'Hello', 'How are you', 'whats up', 'How do you do', 'How’s the day?', 'Hey are you there', 'How is everything going', 'glad to meet you'], 'responses': ['Hello', 'Glad to see you again', 'Hi there, How can I help  you', 'Hello! what you are looking for?', 'Hello, how may I help you']}, {'tag': 'endingnote', 'patterns': ['see you. bye', 'Good Bye', 'ok then bye', 'That’ s enough for me', ' Im leaving', 'See u later! Goodbye', 'nice to meet you', 'The bargaining session was fun!I’m happy that I got it at cheap price, see u again', 'Enjoyed the experience, good bye'], 'responses': ['Hey, Thank you for visiting', 'Hope to see you again, it was nice talking with you', ' wish you a good day! Hope to have a talk with you later', 'Thanks for your time with us.Hope you enjoyed it and satisfied. See you later', 'Sad to see you go. Come whenever you need to bye from us. Stay intouch.']}, {'tag': 'name', 'patterns': ['whats you name?', 'who are you?', 'what can I call you', 'tell me your name', 'Are u a robot', 'Are you a chatbot'], 'responses': ['It’s FinBot here, a digital+financial assistant to help you out', 'You can call me FinBot.Im a chatRobot/chatbot, tell me what you want', 'I’m FinBot,a digital+financial assistant, I can guide you in our ecommerce website.', 'Its FinBot Here,a digital +financial assistant. How may I help you?']}, {'tag': 'shopping', 'patterns': ['I would like to buy something here', 'can I buy something', 'What all things are there', 'can you recommend me something from here', 'is there any thing cheaper here', 'Hey, I want a black shoe'], 'responses': ['Yes please, we have a lot of new collections for you', 'Ofcourse, there are new offers for you, fantastic selections, choose what interests you!', 'Well casual shoes at Rs 220, this offer won’t stay much', 'there are good varieties of coloured shoes too also there are black formal shoes. Take one']}, {'tag': 'time period', 'patterns': ['till what time this be offered?', 'When will the shop close?', 'when will the shop open', 'How much time it will take for delivery', 'When will this site close?', 'When are you people open', 'When will this store be open?', 'Is this store will be open at Sunday?'], 'responses': ['Our shop centre will be open from 9 am-9pm in all the days', 'Yes, we are operating on all the weekdays from 9 am to 9pm', 'We are there to provide services from Monday-sunday from 9 am-9pm', 'We are operating from 9 am-9 pm every day']}, {'tag': 'location', 'patterns': ['What is your location?', 'Where are you located?', 'What is your address?', 'Where is your restaurant situated?'], 'responses': ['We are on the intersection of state Alley and Techno Avenue, Delhi, India', 'We are situated at the intersection of  state Alley and Techno Avenue in Delhi, India', 'Our Address is: 1000 Techno Avenue, Delhi, India']}, {'tag': 'payments', 'patterns': ['Do you take credit cards?', 'Do you accept Mastercard?', 'Are you cash only?', 'will you accept credit card', 'can i pay by credit card'], 'responses': ['We accept VISA, Mastercard and AMEX', 'We accept most major credit cards']}, {'tag': 'todaysOffers', 'patterns': ['What is the new collections for today?', 'What new collections you have today?', \"Tell me today's collection\"], 'responses': ['We have buy one get one offer for kurtis and shirts. Also new colour selections of traditional wears', 'Our speciality for today is trendy new collections of traditional wears and also buy one get one offer for some things']}, {'tag': 'deliveryoption', 'patterns': ['Do you provide delivery to home?', 'Do you deliver the clothes?', 'What are the home delivery options?', 'Tell me the delivery options you have'], 'responses': ['Yes, we provide home delivery of the proiducts through our agents?', 'We have home delivery options through our agents']}, {'tag': 'price of formal wear', 'patterns': ['what is the price for a formal wear', 'How much will it cost to buy a formal wear', 'Will it be much expensive to buy a branded suit, how much is the average cost'], 'responses': ['For  a formal wear it will cost in a range of 750-2500 Rs according to your selection', 'There are a number of varieties of suits or formal wears ranging from 500-1500', 'Its not much expensive to have the same. There are several discounts available', 'it has an average cost of 1200 Rs']}, {'tag': 'price of a saree', 'patterns': ['what is the price for a saree', 'How much will it cost to buy a saree', 'What is the cost of a saree', 'Cost of a saree', 'Price of a saree'], 'responses': ['The price of saree depends upon the colour and design of the same ', 'We have collections of trending sarees ranging from 500Rs to 10000 Rs'], 'context_set': ''}, {'tag': 'Thank you', 'patterns': ['Thank you', 'Thank you so much for your help'], 'responses': ['Its my pleasure.', 'You are welcome', 'No problem. Its my pleasure to help you']}, {'tag': 'Bargain offer', 'patterns': ['Can i bargain for any product today', 'For what product do you have bargaining offer today', 'can you bargain today', 'can I bargain today', 'Which product can I bargain today', 'Tell me is there a product to bargain'], 'responses': ['Yes, we have a bargaining offer for ribbon today. Its rated price is 30 Rs', 'yes, today you can bargaining for a ribbon. its rated price is 30 Rs']}, {'tag': 'bitcoin', 'patterns': ['tell me the value of a bitcoin', 'what is bitcoin', 'Can you tell more about bitcoin?', 'What can be bitcoin', 'what do you mean by bitcoin'], 'responses': ['1 Bitcoin equals 14,14,998.64 Indian Rupee', 'Bitcoin is a digital or virtual currency created in 2009 that uses peer-to-peer technology to facilitate instant payments', 'Bitcoin is a cryptocurrency created in 2009. Marketplaces called “bitcoin exchanges” allow people to buy or sell bitcoins using different currencies']}, {'tag': 'AsymmetricInformation', 'patterns': ['what is Asymmetric Information', 'Tell me about Asymmetric information in finance markets', 'Do you know about Asymmetric information', 'What can be Asymmetric information', 'what do you mean by Asymmetric information'], 'responses': ['It is a type of market failure exists when one individual or party has much more information than another individual or party, and uses that advantage to exploit the other party.', 'Finance is a market in information – often a potential borrower (such as a small business) has better information on the likelihood that they will be able to repay a loan than the lender.']}, {'tag': 'bail-out', 'patterns': ['what is bail-out', 'Do you know about bail-out in financial markets', 'Tell me about bailout', 'What does critics and supporter say about bail-out', 'bail-out'], 'responses': ['A bail-out happens when a government buys an equity stake in a bank or some other form of financial support to prevent it from failing', 'Critics of bail-outs argue that it can increase the burden facing taxpayers and also increase the risk of moral hazard.Supporters of bail-outs claim that they are sometimes necessary during a financial crisis to help reduce systemic risk.', ' Bail outs have become prominent once again because of the economic and social consequences of the coronavirus crisis.']}, {'tag': 'Balance sheet', 'patterns': ['what is a balance sheet', 'Tell me about Balance sheet', 'Do you know Balance sheet?', 'Balance sheet', 'What can be Balance sheet', 'what do you mean by Balance sheet'], 'responses': ['A record of the assets, liabilities, and net worth of an economic actor such as a household, commercial or central bank, firm, or government', \"A balance sheet is a financial statement that reports a company's assets, liabilities and shareholders' equity.\", 'The balance sheet is one of the three (income statement and statement of cash flows being the other two) core financial statements used to evaluate a business.']}, {'tag': 'bank', 'patterns': ['what is a bank', 'What exactly bank does', 'can you tell me about bank', 'bank', 'What can be bank', 'what do you mean by bank'], 'responses': ['A bank in simple terms is a business that makes its profit by paying interest to those who keep money there and charging a higher rate of interest to people/businesses who borrow money from the bank']}, {'tag': 'bank assests', 'patterns': ['what are bank assets', 'Tell me about bank assets', 'what is bank asset', 'bank assets', 'what is mean by bank assets', 'bank assets', 'What can be bank assets', 'what do you mean by bank assets'], 'responses': ['Assets are “owned” by the bank e.g. cash, balances at Bank of England, loans (Advances), securities (e.g. Bonds) and fixed assets.']}, {'tag': 'Bank capital', 'patterns': ['what is mean by bank capital?', 'do you know bank capital?', 'what is bank capital', 'bank capital', 'Tell me about Bank capital', 'What can be Bank capital', 'what do you mean by Bank capital'], 'responses': [\"Bank capital is the value of the bank's assets minus its liabilities, or debts.\"]}, {'tag': 'Bond market', 'patterns': ['what is mean by bond market?', 'do you know bond market?', 'what is bond market?', 'Bond Market', 'What can be Bond Market', 'what do you mean by Bank Market'], 'responses': ['The bond market broadly describes a marketplace where investors buy debt securities that are brought to the market by either governmental entities or publicly-traded corporations', 'The market for interest-bearing securities (with either a fixed or a floating rate and with a maturity of at least one year) that companies and governments issue to raise capital for investment.']}, {'tag': 'Bank Run', 'patterns': ['what is mean by Bank Run?', 'What is bank run and how it occurs?', 'Do you know about bank run?', 'Bank Run', 'What can be bank run', 'what do you mean by bank run'], 'responses': [\"A situation in which depositors withdraw funds from a commercial bank because they fear that it may go bankrupt and not honour its liabilities such as the deposits of the bank's savers.\", \"A bank run occurs when a large number of customers of a bank or other financial institution withdraw their deposits simultaneously over concerns of the bank's solvency.\"]}, {'tag': 'Bank reserves', 'patterns': ['what is mean by bank reserves?', 'do you know bank reserves?', 'Bank reserves and how it works', 'what is bank reserves', 'Bank reserves', 'What can be Bank reserves', 'what do you mean by Bank reserves'], 'responses': ['Money and liquid assets (such as securities that can be sold quickly) held by banks in order to meet withdrawals by customers.', 'Bank reserves are the cash minimums that must be kept on hand by financial institutions in order to meet central bank requirements']}, {'tag': 'Capital market', 'patterns': ['what is mean by a capital market?', 'capital market?', 'do you know Capital market?', 'Tell me about capital market', 'capital market', 'What can be capital market', 'what do you mean by capital market'], 'responses': ['Capital markets are the markets where securities such as shares and bonds are issued to raise medium to long-term financing.']}, {'tag': 'Capital ratio', 'patterns': ['what is mean by capital ratio?', 'do you know Capital ratio?', 'Tell me about capital ratio', 'capital ratio', 'What can be capital ratio', 'what do you mean by capital ratio'], 'responses': [\"A commercial bank's capital ratio measures the funds it has in reserve against the riskier assets it holds that could be vulnerable in the event of a crisis.\", 'the extent to which a financial institution finances its operations by issuing shares and retaining profits, expressed as a percentage of its assets.']}, {'tag': 'Credit risk', 'patterns': ['what is mean by credit risk?', 'do you know credit risk?', 'Tell me about credit risk', 'what is credit risk', 'credit risk', 'What can be credit risk', 'what do you mean by credit risk'], 'responses': [\"Credit risk is the possibility of a loss resulting from a borrower's failure to repay a loan or meet contractual obligations\", 'This is the risk to the commercial bank of lending to borrowers who turn out to be unable to repay their loans.']}, {'tag': 'Crowdfunding', 'patterns': ['what is mean by crowdfunding?', 'do you know crowdfunding?', 'Tell me about crowdfunding', 'what is crowdfunding?', 'crowdfunding', 'What can be crowdfunding', 'what do you mean by crowdfunding'], 'responses': ['Crowdfunding is a form of equity finance that has grown rapidly in the USA and the UK in particular.', 'Crowdfunding involves the collective effort of a large number of individuals who network and pool small amounts of their capital to finance a new or existing business venture. Social causes remain the most active source of crowdfunding activity.', ' Crowdfunding is the use of small amounts of capital from a large number of individuals to finance a new business venture.']}, {'tag': 'cryptocurrency', 'patterns': ['what is mean by cryptocurrency?', 'do you know cryptocurrency?', 'what is cryptocurrency', 'Tell me about cryptocurrency', 'cryptocurrency', 'What can be cryptocurrency', 'what do you mean by cryptocurrency'], 'responses': ['A cryptocurrency is a digital or virtual currency that uses cryptography and is difficult to counterfeit because of this security feature.', 'A cryptocurrency is a digital or virtual currency designed to work as a medium of exchange']}, {'tag': 'Demonetisation', 'patterns': ['what is mean by demonetisation?', 'do you know about demonetization that happened in our India in 2016?', 'what is demonetisation', 'What can be demonetisation', 'what do you mean by demonetisation', 'Tell me about demonetisation', 'Demonetisation in India', 'Demonetisation'], 'responses': ['Yes,Demonetization is the act of stripping a currency unit of its status as legal tender.', 'Demonetisation is a process of removing a unit of currency from circulation by stripping it of its legal tender status', 'Yes, Demonetization occurs when a governing body cancels the legal tender status of a currency unit in circulation.']}, {'tag': 'Financial crisis', 'patterns': ['what is mean by financial crisis in a country?', 'do you know bank capital?', 'Tell me about bank capital', 'bank capital', 'What can be bank capital', 'what do you mean by bank capital'], 'responses': ['A disturbance to financial markets, associated typically with falling asset prices and insolvency amongst debtors and intermediaries, which ramifies through the financial system, disrupting the market’s capacity to allocate capital', 'A financial crisis is a situation where the value of assets drop rapidly and is often triggered by a panic or a run on banks.']}, {'tag': 'liquidity', 'patterns': ['what is mean by liquidity?', 'do you know liquidity in financial markets?', 'what is liquidity?', 'liquidity', 'What can be liquidity', 'what do you mean by liquidity'], 'responses': ['Liquidity means the ease and cost with which assets can be turned into cash and used immediately as a means of exchange. Cash is very liquid whereas a life assurance policy is less so.', 'It is the the availability of liquid assets such as cash to a market or company.']}, {'tag': 'leverage', 'patterns': ['what is mean by leverage?', 'do you know about leverage?', 'do you know leverage?', 'Tell me about leverage', 'leverage', 'What can be leverage', 'what do you mean by leverage'], 'responses': ['Leverage is the use of borrowed funds to increase profitability. One measure of leverage is the amount of long term debt relative to equity', 'Leverage is an investment strategy of using borrowed money—specifically, the use of various financial instruments or borrowed capital—to increase the potential return of an investment', 'Financial leverage which is also known as leverage or trading on equity, refers to the use of debt to acquire additional assets.']}, {'tag': 'liquidity trap', 'patterns': ['what is mean by liquidity trap?', 'do you know liquidity trap?', 'Hey tell me what is liquidity trap', 'what is liquidity trap?', 'liquidity trap', 'What can be liquidity trap', 'what do you mean by liquidity trap'], 'responses': ['A liquidity trap occurs when low interest rates and a high amount of cash balances in the economy fail to stimulate aggregate demand partly through a lack of confidence.']}, {'tag': 'Monetary stability', 'patterns': ['what is mean by monetary stability?', 'do you know about monetary stability?', 'Tell me about what exactly monetary stability is.', 'what is monetary stability', 'Monetary Stability', 'What can be Monitary Stability', 'what do you mean by Monitary stability'], 'responses': ['Monetary stability means stable prices and confidence in the currency. ', 'Monetary stability is a synonym for price stability. Price stability refers to a stable price level or a low level of inflation and not to stable individual prices']}, {'tag': 'Moral hazard', 'patterns': ['what is moral hazard?', 'what is mean by moral hazard?', 'tell me an example of moral hazard?', 'do you know moral hazard', 'moral hazard', 'What can be moral hazard', 'what do you mean by moral hazard'], 'responses': ['When the party with superior information alters his/her behaviour in such a way that benefits himself while imposing costs on those with inferior information.', 'For example, moral hazard occurs when insured consumers are likely to take greater risks, knowing that a claim will be paid for by their cover. The consumer knows more about his/her intended actions than the producer (e.g. the insurer).']}, {'tag': 'nominal interest rate', 'patterns': ['what is mean by nominal interest rate?', 'do you know nominal interest rate?', 'what is nominal interest rate?', 'Tell me about nominal interest rate', 'nominal interest rate', 'What can be nominal interest rate', 'what do you mean by nominal interest rate'], 'responses': ['The nominal interest rate is the interest rate on a loan or on savings deposits unadjusted for the rate of inflation.', 'In short, it is the sum of real interest rate and inflation']}, {'tag': 'real interest rate', 'patterns': ['what is mean by real interest rate?', 'do you know real interst rate?', 'what is real interest rate', 'Tell me about real interest rate', 'real interest rate', 'what can be real interest rate?', 'What do you mean by real interest rate'], 'responses': ['A real interest rate is an interest rate that has been adjusted to remove the effects of inflation to reflect the real cost of funds to the borrower and the real yield to the lender or to an investor.', 'The real interest rate reflects the rate of time-preference for current goods over future goods.']}, {'tag': 'narrow money', 'patterns': ['what is mean by narrow money?', 'do you know narrow money in financial markets?', 'what is narrow money', 'Tell me about Narrow money', 'Narrow money', 'what can be Narrow money', 'What do you mean by Narrow money'], 'responses': ['Narrow money refers to a category of money supply that includes all the real money held by the central bank. It includes coins and currency, demand deposits, and other liquid assets.', 'The narrow money definition of the money supply is a measure of the value coins and notes in circulation and other money equivalents that are easily convertible into cash such as short term deposits in the banking system.']}, {'tag': 'passporting', 'patterns': ['what is mean by pass porting?', 'do you know passporting?', 'what is passporting', 'Tell me about passporting', 'passporting', 'What can be passporting', 'what do you mean by passporting'], 'responses': ['Passporting allows a firm registered in the European Economic Area (EEA) to do business in any other EEA state without the need for further authorization from each country.', 'EU legislation gives UK banks, insurers and investment firms ‘passporting’ rights to provide a range of financial services to clients across the EU – either cross-border or through local branches – while remaining regulated solely by their ‘home’ state.']}, {'tag': 'PRA', 'patterns': ['what is mean by Prudential Regulation Authority or PRA?', 'do you know PRA?', 'what is PRA', 'PRA', 'what can be PRA'], 'responses': ['The PRA is part of the Bank of England and is responsible for the prudential regulation and supervision of around 1,700 banks, building societies, credit unions, insurers and major investment firms.', 'The PRA has a particular focus on the solvency of specific financial markets such as: Insurance providers, Buy-to-let mortgage lenders, Credit unions and other specialist lenders.']}, {'tag': 'Ring Fence', 'patterns': ['what is mean by ring-fence?', 'do you know ring fence?', 'Ring fencing?', 'what is ring fence', 'Tell me about ring fence', 'Ring Fence'], 'responses': [\"A ring-fence is a virtual barrier that segregates a portion of an individual's or company's financial assets from the rest.\", 'Ringfencing is when a regulated public utility business financially separates itself from a parent company that engages in non-regulated business.']}, {'tag': 'VAR', 'patterns': ['what is mean by Value at risk?', 'do you know VAR?', 'what is exactly VAR?', 'what is VAR', 'Tell me about VAR', 'Do you know Value at Risk', 'VAR', 'Value at Risk'], 'responses': ['Value at risk is a measure of the maximum loss expected on an investment.', 'For example, a 2% value at risk of a £3 million investment means that there is a 2% chance that a portfolio will lose £3 million in a given year.']}, {'tag': 'Compound interest', 'patterns': ['What is mean by Compound interest', 'Compound interest', 'what is compound interest', 'Tell me about compound interest', 'Do you know compound interest?', 'What can be compound interest', 'What do you mean by compound interest'], 'responses': ['It is the interest on the amount of money you have deposited or borrowed', 'When you are investing or saving, compound interest is charged on the original amount you have accumulated over time']}, {'tag': 'FICO score', 'patterns': ['What is Fico Score', 'What is mean by Fico Score', 'Fico Score means?', 'Fico score', 'Do you know Fico Score', 'Tell me about Fico Score', 'What can be Fico score'], 'responses': ['Your Fico score is based on several factors including payment history, the length of your credit history and total amount owed', 'Fico Score Reanges from 300 to 850, and the higher the score,the better the terms you may receive on your next loan or credit card.', 'People with Fico scores below 620 may have a harder time securing credit at a favourable interest rate']}, {'tag': 'Net Worth', 'patterns': ['What is Net Worth', 'What is mean by Net Worth', 'net worth means?', 'Net worth', 'Do you know Net Worth', 'Tell me about Net worth', 'What does Net worth do?', 'What do you mean by Net worth', 'what can be networth'], 'responses': ['Your networth is simply the difference between your assets']}, {'tag': 'Asset allocation', 'patterns': ['What is Asset allocation', 'What is mean by Asset allocation', 'Asset allocation means?', 'Asset allocation', 'Do you know Asset allocation', 'Tell me about Asset allocation', 'What does Asset allocation do?', 'What `do you mean by Asset allocation', 'what can be Asset allocation'], 'responses': ['Asset allocation is where you choose to put your money', 'The three major asset classes are stocks, bonds, and cash']}, {'tag': 'Term life insurance', 'patterns': ['What is term life insurance', 'Do you know term life insurance', 'what is mean by term life insurance', 'Tell me about term life insurance'], 'responses': ['Term life insurance provide coverage over a set period, generally anywhere from five to 30 years', \"If you die within the set term, your beneficiaries receive a payout. If you don't,the policy expires with no value.\"]}, {'tag': 'Thank you', 'patterns': ['Thank you', 'Thank you so much for your help'], 'responses': ['Its my pleasure.', 'You are welcome', 'No problem. Its my pleasure to help you']}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDP8d1e6uNCF",
        "outputId": "1b9d70f2-d3f6-4230-8de5-c1c3100e137d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install graphviz\n",
        "!pip3 install torchviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBVdwf67ujfW",
        "outputId": "b7a01a23-93e9-4659-b3cb-68dfbb46505e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.3.1+cu121)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->torchviz) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot,make_dot_from_trace"
      ],
      "metadata": {
        "id": "fupTGgC0u7Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentence):\n",
        "    return nltk.word_tokenize(sentence)"
      ],
      "metadata": {
        "id": "7UjD7Z-Mu9u7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "def stem(word):\n",
        "    return stemmer.stem(word.lower())"
      ],
      "metadata": {
        "id": "jHhfGPmCu-MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def BagOfWords(tokenized_sentence, words):\n",
        "    sentence_words = [stem(word) for word in tokenized_sentence]\n",
        "    Bag = np.zeros(len(words), dtype=np.float32)\n",
        "    for idx, w in enumerate(words):\n",
        "        if w in sentence_words:\n",
        "            Bag[idx] = 1\n",
        "    return Bag"
      ],
      "metadata": {
        "id": "1iNj6xBvvAGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=[\"hi\",\"im\",\"ola\",\"hello\"]\n",
        "words=[\"hi\",\"how\",\"are\",\"you\"]\n",
        "BagOfWords(sentence,words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfKQLwEGvBWR",
        "outputId": "712df6d0-fd7a-475a-8bbc-54f10ebf9a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_the_words = []\n",
        "\n",
        "tags = []\n",
        "\n",
        "pair = []\n",
        "\n",
        "for intent in intents['intents']:\n",
        "    tag = intent['tag']\n",
        "\n",
        "    tags.append(tag)\n",
        "    for pattern in intent['patterns']:\n",
        "        w = tokenize(pattern)\n",
        "\n",
        "        all_the_words.extend(w)\n",
        "\n",
        "        pair.append((w, tag))\n",
        "\n",
        "ignore_words = ['?', '.', '!',',']\n",
        "all_the_words = [stem(w) for w in all_the_words if w not in ignore_words]\n",
        "\n",
        "all_the_words = sorted(set(all_the_words))\n",
        "tags = sorted(set(tags))\n",
        "\n",
        "print(len(pair), \"patterns\")\n",
        "print(len(tags), \"tags:\", tags)\n",
        "print(len(all_the_words), \"unique stemmed words:\", all_the_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTAZgaFTvDZ9",
        "outputId": "b1495deb-62fd-43a8-826c-38e1beaf1454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "298 patterns\n",
            "47 tags: ['Asset allocation', 'AsymmetricInformation', 'Balance sheet', 'Bank Run', 'Bank capital', 'Bank reserves', 'Bargain offer', 'Bond market', 'Capital market', 'Capital ratio', 'Compound interest', 'Credit risk', 'Crowdfunding', 'Demonetisation', 'FICO score', 'Financial crisis', 'Monetary stability', 'Moral hazard', 'Net Worth', 'PRA', 'Ring Fence', 'Term life insurance', 'Thank you', 'VAR', 'bail-out', 'bank', 'bank assests', 'bitcoin', 'cryptocurrency', 'deliveryoption', 'endingnote', 'leverage', 'liquidity', 'liquidity trap', 'location', 'name', 'narrow money', 'nominal interest rate', 'passporting', 'payments', 'price of a saree', 'price of formal wear', 'real interest rate', 'shopping', 'time period', 'todaysOffers', 'welcome']\n",
            "209 unique stemmed words: [\"'s\", '2016', '`', 'a', 'about', 'accept', 'address', 'again', 'all', 'alloc', 'an', 'and', 'ani', 'are', 'asset', 'asymmetr', 'at', 'author', 'averag', 'bail-out', 'bailout', 'balanc', 'bank', 'bargain', 'be', 'bitcoin', 'black', 'bond', 'brand', 'buy', 'by', 'bye', 'call', 'can', 'capit', 'card', 'cash', 'chatbot', 'cheap', 'cheaper', 'close', 'cloth', 'collect', 'compound', 'cost', 'countri', 'credit', 'crisi', 'critic', 'crowdfund', 'cryptocurr', 'day', 'deliv', 'deliveri', 'demonet', 'demonetis', 'do', 'doe', 'enjoy', 'enough', 'everyth', 'exactli', 'exampl', 'expens', 'experi', 'fenc', 'fico', 'financ', 'financi', 'for', 'formal', 'from', 'fun', 'glad', 'go', 'good', 'goodby', 'got', 'happen', 'happi', 'have', 'hazard', 'hello', 'help', 'here', 'hey', 'hi', 'home', 'how', 'i', 'im', 'in', 'india', 'inform', 'insur', 'interest', 'interst', 'is', 'it', 'know', 'later', 'leav', 'leverag', 'life', 'like', 'liquid', 'locat', 'm', 'market', 'mastercard', 'me', 'mean', 'meet', 'monetari', 'money', 'monitari', 'moral', 'more', 'much', 'name', 'narrow', 'net', 'networth', 'new', 'nice', 'nomin', 'occur', 'of', 'offer', 'ok', 'onli', 'open', 'option', 'or', 'our', 'pass', 'passport', 'pay', 'peopl', 'port', 'pra', 'price', 'product', 'provid', 'prudenti', 'rate', 'ratio', 'real', 'recommend', 'regul', 'reserv', 'restaur', 'ring', 'ring-fenc', 'risk', 'robot', 'run', 's', 'sare', 'say', 'score', 'see', 'session', 'sheet', 'shoe', 'shop', 'site', 'situat', 'so', 'someth', 'stabil', 'store', 'suit', 'sunday', 'support', 'take', 'tell', 'term', 'thank', 'that', 'the', 'then', 'there', 'thi', 'thing', 'till', 'time', 'to', 'today', 'trap', 'u', 'up', 'valu', 'var', 'wa', 'want', 'wear', 'what', 'when', 'where', 'which', 'who', 'will', 'work', 'worth', 'would', 'you', 'your', '’']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "for (pattern_sentence, tag) in pair:\n",
        "    bag = BagOfWords(pattern_sentence, all_the_words)\n",
        "    x_train.append(bag)\n",
        "    label = tags.index(tag)\n",
        "    y_train.append(label)\n",
        "#array\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "num_epochs = 1000\n",
        "batch_size = 8\n",
        "learning_rate = 0.001\n",
        "\n",
        "input_size = len(x_train[0])\n",
        "hidden_size = 8\n",
        "output_size = len(tags)\n",
        "\n",
        "print(\"inputsize=\",input_size)\n",
        "print(\"outputsize=\",output_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3hx-K9CvLE1",
        "outputId": "f62f7edf-11a7-4b89-d24b-872478c50ba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputsize= 209\n",
            "outputsize= 47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.n_samples = len(x_train)\n",
        "        self.x_data = x_train\n",
        "        self.y_data = y_train\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples"
      ],
      "metadata": {
        "id": "9LdWNbx8vM14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNetModel, self).__init__()\n",
        "\n",
        "        self.linearlayer1 = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        self.bn1= nn.BatchNorm1d(hidden_size)\n",
        "\n",
        "        self.linearlayer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.bn2= nn.BatchNorm1d(hidden_size)\n",
        "\n",
        "        self.linearlayer3 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        output = self.linearlayer1(x)\n",
        "\n",
        "        output = self.relu(output)\n",
        "\n",
        "        output = self.linearlayer2(output)\n",
        "\n",
        "        output = self.relu(output)\n",
        "        output = self.linearlayer3(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "B41FGlJrvOdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NeuralNetModel(input_size, hidden_size, output_size).to(device)"
      ],
      "metadata": {
        "id": "CYP8iA4gvQj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.randn(47,209)\n",
        "y=model(x)\n",
        "make_dot(y.mean(),params=dict(model.named_parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "id": "b21bMhNPvT71",
        "outputId": "c9649ac8-2b13-473e-e25d-206e366248df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"487pt\" height=\"600pt\"\n viewBox=\"0.00 0.00 487.00 600.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 596)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-596 483,-596 483,4 -4,4\"/>\n<!-- 139071639129440 -->\n<g id=\"node1\" class=\"node\">\n<title>139071639129440</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"241.5,-31 187.5,-31 187.5,0 241.5,0 241.5,-31\"/>\n<text text-anchor=\"middle\" x=\"214.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 139071641581072 -->\n<g id=\"node2\" class=\"node\">\n<title>139071641581072</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"262,-86 167,-86 167,-67 262,-67 262,-86\"/>\n<text text-anchor=\"middle\" x=\"214.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">MeanBackward0</text>\n</g>\n<!-- 139071641581072&#45;&gt;139071639129440 -->\n<g id=\"edge21\" class=\"edge\">\n<title>139071641581072&#45;&gt;139071639129440</title>\n<path fill=\"none\" stroke=\"black\" d=\"M214.5,-66.79C214.5,-60.07 214.5,-50.4 214.5,-41.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"218,-41.19 214.5,-31.19 211,-41.19 218,-41.19\"/>\n</g>\n<!-- 139071641579920 -->\n<g id=\"node3\" class=\"node\">\n<title>139071641579920</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"265,-141 164,-141 164,-122 265,-122 265,-141\"/>\n<text text-anchor=\"middle\" x=\"214.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 139071641579920&#45;&gt;139071641581072 -->\n<g id=\"edge1\" class=\"edge\">\n<title>139071641579920&#45;&gt;139071641581072</title>\n<path fill=\"none\" stroke=\"black\" d=\"M214.5,-121.75C214.5,-114.8 214.5,-104.85 214.5,-96.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"218,-96.09 214.5,-86.09 211,-96.09 218,-96.09\"/>\n</g>\n<!-- 139071641580400 -->\n<g id=\"node4\" class=\"node\">\n<title>139071641580400</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"143,-196 42,-196 42,-177 143,-177 143,-196\"/>\n<text text-anchor=\"middle\" x=\"92.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 139071641580400&#45;&gt;139071641579920 -->\n<g id=\"edge2\" class=\"edge\">\n<title>139071641580400&#45;&gt;139071641579920</title>\n<path fill=\"none\" stroke=\"black\" d=\"M112.1,-176.98C131.82,-168.42 162.45,-155.11 185.08,-145.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"186.76,-148.37 194.54,-141.17 183.98,-141.94 186.76,-148.37\"/>\n</g>\n<!-- 139071639126960 -->\n<g id=\"node5\" class=\"node\">\n<title>139071639126960</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"146,-262 27,-262 27,-232 146,-232 146,-262\"/>\n<text text-anchor=\"middle\" x=\"86.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\">linearlayer3.bias</text>\n<text text-anchor=\"middle\" x=\"86.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\"> (47)</text>\n</g>\n<!-- 139071639126960&#45;&gt;139071641580400 -->\n<g id=\"edge3\" class=\"edge\">\n<title>139071639126960&#45;&gt;139071641580400</title>\n<path fill=\"none\" stroke=\"black\" d=\"M87.95,-231.84C88.73,-224.21 89.71,-214.7 90.56,-206.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"94.06,-206.57 91.6,-196.27 87.1,-205.86 94.06,-206.57\"/>\n</g>\n<!-- 139071641580016 -->\n<g id=\"node6\" class=\"node\">\n<title>139071641580016</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"262,-196 167,-196 167,-177 262,-177 262,-196\"/>\n<text text-anchor=\"middle\" x=\"214.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n</g>\n<!-- 139071641580016&#45;&gt;139071641579920 -->\n<g id=\"edge4\" class=\"edge\">\n<title>139071641580016&#45;&gt;139071641579920</title>\n<path fill=\"none\" stroke=\"black\" d=\"M214.5,-176.75C214.5,-169.8 214.5,-159.85 214.5,-151.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"218,-151.09 214.5,-141.09 211,-151.09 218,-151.09\"/>\n</g>\n<!-- 139071641580256 -->\n<g id=\"node7\" class=\"node\">\n<title>139071641580256</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"265,-256.5 164,-256.5 164,-237.5 265,-237.5 265,-256.5\"/>\n<text text-anchor=\"middle\" x=\"214.5\" y=\"-244.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 139071641580256&#45;&gt;139071641580016 -->\n<g id=\"edge5\" class=\"edge\">\n<title>139071641580256&#45;&gt;139071641580016</title>\n<path fill=\"none\" stroke=\"black\" d=\"M214.5,-237.37C214.5,-229.25 214.5,-216.81 214.5,-206.39\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"218,-206.17 214.5,-196.17 211,-206.17 218,-206.17\"/>\n</g>\n<!-- 139071641580592 -->\n<g id=\"node8\" class=\"node\">\n<title>139071641580592</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"116,-322.5 15,-322.5 15,-303.5 116,-303.5 116,-322.5\"/>\n<text text-anchor=\"middle\" x=\"65.5\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 139071641580592&#45;&gt;139071641580256 -->\n<g id=\"edge6\" class=\"edge\">\n<title>139071641580592&#45;&gt;139071641580256</title>\n<path fill=\"none\" stroke=\"black\" d=\"M85.59,-303.37C111.14,-292.39 155.46,-273.36 184.89,-260.72\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"186.44,-263.86 194.25,-256.7 183.68,-257.43 186.44,-263.86\"/>\n</g>\n<!-- 139071639126320 -->\n<g id=\"node9\" class=\"node\">\n<title>139071639126320</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"119,-394 0,-394 0,-364 119,-364 119,-394\"/>\n<text text-anchor=\"middle\" x=\"59.5\" y=\"-382\" font-family=\"monospace\" font-size=\"10.00\">linearlayer2.bias</text>\n<text text-anchor=\"middle\" x=\"59.5\" y=\"-371\" font-family=\"monospace\" font-size=\"10.00\"> (8)</text>\n</g>\n<!-- 139071639126320&#45;&gt;139071641580592 -->\n<g id=\"edge7\" class=\"edge\">\n<title>139071639126320&#45;&gt;139071641580592</title>\n<path fill=\"none\" stroke=\"black\" d=\"M60.83,-363.8C61.68,-354.7 62.8,-342.79 63.73,-332.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"67.22,-333.13 64.67,-322.84 60.25,-332.47 67.22,-333.13\"/>\n</g>\n<!-- 139071641581312 -->\n<g id=\"node10\" class=\"node\">\n<title>139071641581312</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"235,-322.5 140,-322.5 140,-303.5 235,-303.5 235,-322.5\"/>\n<text text-anchor=\"middle\" x=\"187.5\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n</g>\n<!-- 139071641581312&#45;&gt;139071641580256 -->\n<g id=\"edge8\" class=\"edge\">\n<title>139071641581312&#45;&gt;139071641580256</title>\n<path fill=\"none\" stroke=\"black\" d=\"M191.14,-303.37C195.14,-293.88 201.69,-278.36 206.84,-266.16\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"210.08,-267.48 210.74,-256.91 203.63,-264.76 210.08,-267.48\"/>\n</g>\n<!-- 139071641573104 -->\n<g id=\"node11\" class=\"node\">\n<title>139071641573104</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"238,-388.5 137,-388.5 137,-369.5 238,-369.5 238,-388.5\"/>\n<text text-anchor=\"middle\" x=\"187.5\" y=\"-376.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 139071641573104&#45;&gt;139071641581312 -->\n<g id=\"edge9\" class=\"edge\">\n<title>139071641573104&#45;&gt;139071641581312</title>\n<path fill=\"none\" stroke=\"black\" d=\"M187.5,-369.37C187.5,-360.16 187.5,-345.29 187.5,-333.27\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"191,-332.91 187.5,-322.91 184,-332.91 191,-332.91\"/>\n</g>\n<!-- 139071641579728 -->\n<g id=\"node12\" class=\"node\">\n<title>139071641579728</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"121,-454.5 20,-454.5 20,-435.5 121,-435.5 121,-454.5\"/>\n<text text-anchor=\"middle\" x=\"70.5\" y=\"-442.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 139071641579728&#45;&gt;139071641573104 -->\n<g id=\"edge10\" class=\"edge\">\n<title>139071641579728&#45;&gt;139071641573104</title>\n<path fill=\"none\" stroke=\"black\" d=\"M86.28,-435.37C105.91,-424.63 139.64,-406.18 162.73,-393.55\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"164.5,-396.57 171.6,-388.7 161.14,-390.43 164.5,-396.57\"/>\n</g>\n<!-- 139071641452224 -->\n<g id=\"node13\" class=\"node\">\n<title>139071641452224</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"125,-526 6,-526 6,-496 125,-496 125,-526\"/>\n<text text-anchor=\"middle\" x=\"65.5\" y=\"-514\" font-family=\"monospace\" font-size=\"10.00\">linearlayer1.bias</text>\n<text text-anchor=\"middle\" x=\"65.5\" y=\"-503\" font-family=\"monospace\" font-size=\"10.00\"> (8)</text>\n</g>\n<!-- 139071641452224&#45;&gt;139071641579728 -->\n<g id=\"edge11\" class=\"edge\">\n<title>139071641452224&#45;&gt;139071641579728</title>\n<path fill=\"none\" stroke=\"black\" d=\"M66.61,-495.8C67.32,-486.7 68.25,-474.79 69.02,-464.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"72.52,-465.09 69.81,-454.84 65.54,-464.54 72.52,-465.09\"/>\n</g>\n<!-- 139071641571040 -->\n<g id=\"node14\" class=\"node\">\n<title>139071641571040</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"226,-454.5 149,-454.5 149,-435.5 226,-435.5 226,-454.5\"/>\n<text text-anchor=\"middle\" x=\"187.5\" y=\"-442.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 139071641571040&#45;&gt;139071641573104 -->\n<g id=\"edge12\" class=\"edge\">\n<title>139071641571040&#45;&gt;139071641573104</title>\n<path fill=\"none\" stroke=\"black\" d=\"M187.5,-435.37C187.5,-426.16 187.5,-411.29 187.5,-399.27\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"191,-398.91 187.5,-388.91 184,-398.91 191,-398.91\"/>\n</g>\n<!-- 139071641572048 -->\n<g id=\"node15\" class=\"node\">\n<title>139071641572048</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"244,-520.5 143,-520.5 143,-501.5 244,-501.5 244,-520.5\"/>\n<text text-anchor=\"middle\" x=\"193.5\" y=\"-508.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 139071641572048&#45;&gt;139071641571040 -->\n<g id=\"edge13\" class=\"edge\">\n<title>139071641572048&#45;&gt;139071641571040</title>\n<path fill=\"none\" stroke=\"black\" d=\"M192.69,-501.37C191.82,-492.07 190.4,-476.98 189.27,-464.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"192.75,-464.53 188.33,-454.91 185.78,-465.19 192.75,-464.53\"/>\n</g>\n<!-- 139071641566672 -->\n<g id=\"node16\" class=\"node\">\n<title>139071641566672</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"259,-592 128,-592 128,-562 259,-562 259,-592\"/>\n<text text-anchor=\"middle\" x=\"193.5\" y=\"-580\" font-family=\"monospace\" font-size=\"10.00\">linearlayer1.weight</text>\n<text text-anchor=\"middle\" x=\"193.5\" y=\"-569\" font-family=\"monospace\" font-size=\"10.00\"> (8, 209)</text>\n</g>\n<!-- 139071641566672&#45;&gt;139071641572048 -->\n<g id=\"edge14\" class=\"edge\">\n<title>139071641566672&#45;&gt;139071641572048</title>\n<path fill=\"none\" stroke=\"black\" d=\"M193.5,-561.8C193.5,-552.7 193.5,-540.79 193.5,-530.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"197,-530.84 193.5,-520.84 190,-530.84 197,-530.84\"/>\n</g>\n<!-- 139071641580208 -->\n<g id=\"node17\" class=\"node\">\n<title>139071641580208</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"330,-322.5 253,-322.5 253,-303.5 330,-303.5 330,-322.5\"/>\n<text text-anchor=\"middle\" x=\"291.5\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 139071641580208&#45;&gt;139071641580256 -->\n<g id=\"edge15\" class=\"edge\">\n<title>139071641580208&#45;&gt;139071641580256</title>\n<path fill=\"none\" stroke=\"black\" d=\"M281.12,-303.37C268.78,-293.12 247.99,-275.84 232.91,-263.31\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"235.14,-260.61 225.21,-256.91 230.67,-265.99 235.14,-260.61\"/>\n</g>\n<!-- 139071641582464 -->\n<g id=\"node18\" class=\"node\">\n<title>139071641582464</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"358,-388.5 257,-388.5 257,-369.5 358,-369.5 358,-388.5\"/>\n<text text-anchor=\"middle\" x=\"307.5\" y=\"-376.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 139071641582464&#45;&gt;139071641580208 -->\n<g id=\"edge16\" class=\"edge\">\n<title>139071641582464&#45;&gt;139071641580208</title>\n<path fill=\"none\" stroke=\"black\" d=\"M305.34,-369.37C303.02,-360.07 299.24,-344.98 296.22,-332.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"299.55,-331.76 293.73,-322.91 292.76,-333.46 299.55,-331.76\"/>\n</g>\n<!-- 139071639130960 -->\n<g id=\"node19\" class=\"node\">\n<title>139071639130960</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"375,-460 244,-460 244,-430 375,-430 375,-460\"/>\n<text text-anchor=\"middle\" x=\"309.5\" y=\"-448\" font-family=\"monospace\" font-size=\"10.00\">linearlayer2.weight</text>\n<text text-anchor=\"middle\" x=\"309.5\" y=\"-437\" font-family=\"monospace\" font-size=\"10.00\"> (8, 8)</text>\n</g>\n<!-- 139071639130960&#45;&gt;139071641582464 -->\n<g id=\"edge17\" class=\"edge\">\n<title>139071639130960&#45;&gt;139071641582464</title>\n<path fill=\"none\" stroke=\"black\" d=\"M309.06,-429.8C308.77,-420.7 308.4,-408.79 308.09,-398.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"311.59,-398.73 307.78,-388.84 304.59,-398.95 311.59,-398.73\"/>\n</g>\n<!-- 139071641580064 -->\n<g id=\"node20\" class=\"node\">\n<title>139071641580064</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"391,-196 314,-196 314,-177 391,-177 391,-196\"/>\n<text text-anchor=\"middle\" x=\"352.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 139071641580064&#45;&gt;139071641579920 -->\n<g id=\"edge18\" class=\"edge\">\n<title>139071641580064&#45;&gt;139071641579920</title>\n<path fill=\"none\" stroke=\"black\" d=\"M330.33,-176.98C307.73,-168.3 272.44,-154.75 246.75,-144.88\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"247.67,-141.49 237.08,-141.17 245.16,-148.02 247.67,-141.49\"/>\n</g>\n<!-- 139071641571088 -->\n<g id=\"node21\" class=\"node\">\n<title>139071641571088</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"437,-256.5 336,-256.5 336,-237.5 437,-237.5 437,-256.5\"/>\n<text text-anchor=\"middle\" x=\"386.5\" y=\"-244.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 139071641571088&#45;&gt;139071641580064 -->\n<g id=\"edge19\" class=\"edge\">\n<title>139071641571088&#45;&gt;139071641580064</title>\n<path fill=\"none\" stroke=\"black\" d=\"M381.48,-237.37C376.56,-228.9 368.91,-215.74 362.71,-205.07\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"365.59,-203.05 357.54,-196.17 359.54,-206.57 365.59,-203.05\"/>\n</g>\n<!-- 139071639127200 -->\n<g id=\"node22\" class=\"node\">\n<title>139071639127200</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"479,-328 348,-328 348,-298 479,-298 479,-328\"/>\n<text text-anchor=\"middle\" x=\"413.5\" y=\"-316\" font-family=\"monospace\" font-size=\"10.00\">linearlayer3.weight</text>\n<text text-anchor=\"middle\" x=\"413.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\"> (47, 8)</text>\n</g>\n<!-- 139071639127200&#45;&gt;139071641571088 -->\n<g id=\"edge20\" class=\"edge\">\n<title>139071639127200&#45;&gt;139071641571088</title>\n<path fill=\"none\" stroke=\"black\" d=\"M407.51,-297.8C403.59,-288.5 398.43,-276.27 394.2,-266.26\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"397.34,-264.7 390.23,-256.84 390.89,-267.42 397.34,-264.7\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7e7c23cb7310>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "count_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCvGab5XvWM5",
        "outputId": "8f0b1a60-df74-4fe3-8f6f-02bb775dcbe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2207"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ChatDataset()\n",
        "train_loader = DataLoader(dataset=dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NeuralNetModel(input_size, hidden_size, output_size).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "total=0\n",
        "correct=0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for (words, labels) in train_loader:\n",
        "        words = words.to(device)\n",
        "        labels = labels.to(dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "        outputs = model(words)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scores = model(words)\n",
        "        _, pred = scores.max(1)\n",
        "        total += len(words)\n",
        "        correct += (pred==labels).sum()\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f} And Got {correct} / {total} with accuracy {float(correct)/float(total)*100:.2f}')\n",
        "print(f'final Accuracy-----> Got {correct} / {total} with accuracy {float(correct)/float(total)*100:.2f}')\n",
        "print(f'final loss: {loss.item():.4f}')\n",
        "\n",
        "data = {\n",
        "\"model_state\": model.state_dict(),\n",
        "\"input_size\": input_size,\n",
        "\"hidden_size\": hidden_size,\n",
        "\"output_size\": output_size,\n",
        "\"all_words\": all_the_words,\n",
        "\"tags\": tags\n",
        "}\n",
        "\n",
        "FILE = \"DATA.pth\"\n",
        "torch.save(data, FILE)\n",
        "\n",
        "print(f'training complete. file saved to {FILE}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6s7Tbz6vYNC",
        "outputId": "53fe4ff0-5302-4cab-8c7d-711acd3215dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 3.4500 And Got 163 / 2980 with accuracy 5.47\n",
            "Epoch [20/1000], Loss: 3.7642 And Got 716 / 5960 with accuracy 12.01\n",
            "Epoch [30/1000], Loss: 1.4050 And Got 1440 / 8940 with accuracy 16.11\n",
            "Epoch [40/1000], Loss: 1.7217 And Got 2654 / 11920 with accuracy 22.27\n",
            "Epoch [50/1000], Loss: 1.0401 And Got 4629 / 14900 with accuracy 31.07\n",
            "Epoch [60/1000], Loss: 0.2186 And Got 7253 / 17880 with accuracy 40.56\n",
            "Epoch [70/1000], Loss: 0.1823 And Got 10057 / 20860 with accuracy 48.21\n",
            "Epoch [80/1000], Loss: 0.0652 And Got 12944 / 23840 with accuracy 54.30\n",
            "Epoch [90/1000], Loss: 0.0394 And Got 15869 / 26820 with accuracy 59.17\n",
            "Epoch [100/1000], Loss: 0.1252 And Got 18798 / 29800 with accuracy 63.08\n",
            "Epoch [110/1000], Loss: 0.0085 And Got 21724 / 32780 with accuracy 66.27\n",
            "Epoch [120/1000], Loss: 0.0507 And Got 24654 / 35760 with accuracy 68.94\n",
            "Epoch [130/1000], Loss: 0.0045 And Got 27580 / 38740 with accuracy 71.19\n",
            "Epoch [140/1000], Loss: 0.0178 And Got 30503 / 41720 with accuracy 73.11\n",
            "Epoch [150/1000], Loss: 0.0082 And Got 33426 / 44700 with accuracy 74.78\n",
            "Epoch [160/1000], Loss: 0.0039 And Got 36345 / 47680 with accuracy 76.23\n",
            "Epoch [170/1000], Loss: 0.0013 And Got 39264 / 50660 with accuracy 77.50\n",
            "Epoch [180/1000], Loss: 0.0010 And Got 42185 / 53640 with accuracy 78.64\n",
            "Epoch [190/1000], Loss: 0.0001 And Got 45109 / 56620 with accuracy 79.67\n",
            "Epoch [200/1000], Loss: 0.0010 And Got 48032 / 59600 with accuracy 80.59\n",
            "Epoch [210/1000], Loss: 0.0014 And Got 50953 / 62580 with accuracy 81.42\n",
            "Epoch [220/1000], Loss: 0.0039 And Got 53878 / 65560 with accuracy 82.18\n",
            "Epoch [230/1000], Loss: 0.0004 And Got 56797 / 68540 with accuracy 82.87\n",
            "Epoch [240/1000], Loss: 0.0000 And Got 59718 / 71520 with accuracy 83.50\n",
            "Epoch [250/1000], Loss: 0.0008 And Got 62637 / 74500 with accuracy 84.08\n",
            "Epoch [260/1000], Loss: 0.0004 And Got 65558 / 77480 with accuracy 84.61\n",
            "Epoch [270/1000], Loss: 0.0002 And Got 68482 / 80460 with accuracy 85.11\n",
            "Epoch [280/1000], Loss: 0.0002 And Got 71406 / 83440 with accuracy 85.58\n",
            "Epoch [290/1000], Loss: 0.0005 And Got 74332 / 86420 with accuracy 86.01\n",
            "Epoch [300/1000], Loss: 0.0001 And Got 77256 / 89400 with accuracy 86.42\n",
            "Epoch [310/1000], Loss: 0.0006 And Got 80174 / 92380 with accuracy 86.79\n",
            "Epoch [320/1000], Loss: 0.0004 And Got 83091 / 95360 with accuracy 87.13\n",
            "Epoch [330/1000], Loss: 0.0005 And Got 86015 / 98340 with accuracy 87.47\n",
            "Epoch [340/1000], Loss: 0.0000 And Got 88938 / 101320 with accuracy 87.78\n",
            "Epoch [350/1000], Loss: 0.0000 And Got 91863 / 104300 with accuracy 88.08\n",
            "Epoch [360/1000], Loss: 0.0001 And Got 94785 / 107280 with accuracy 88.35\n",
            "Epoch [370/1000], Loss: 0.0000 And Got 97707 / 110260 with accuracy 88.62\n",
            "Epoch [380/1000], Loss: 0.0000 And Got 100631 / 113240 with accuracy 88.87\n",
            "Epoch [390/1000], Loss: 0.0001 And Got 103547 / 116220 with accuracy 89.10\n",
            "Epoch [400/1000], Loss: 0.3862 And Got 106468 / 119200 with accuracy 89.32\n",
            "Epoch [410/1000], Loss: 0.0000 And Got 109390 / 122180 with accuracy 89.53\n",
            "Epoch [420/1000], Loss: 0.0000 And Got 112314 / 125160 with accuracy 89.74\n",
            "Epoch [430/1000], Loss: 0.0000 And Got 115238 / 128140 with accuracy 89.93\n",
            "Epoch [440/1000], Loss: 0.0000 And Got 118160 / 131120 with accuracy 90.12\n",
            "Epoch [450/1000], Loss: 0.0000 And Got 121082 / 134100 with accuracy 90.29\n",
            "Epoch [460/1000], Loss: 0.0000 And Got 124007 / 137080 with accuracy 90.46\n",
            "Epoch [470/1000], Loss: 0.0000 And Got 126928 / 140060 with accuracy 90.62\n",
            "Epoch [480/1000], Loss: 0.0000 And Got 129849 / 143040 with accuracy 90.78\n",
            "Epoch [490/1000], Loss: 0.0000 And Got 132773 / 146020 with accuracy 90.93\n",
            "Epoch [500/1000], Loss: 0.0000 And Got 135690 / 149000 with accuracy 91.07\n",
            "Epoch [510/1000], Loss: 0.0000 And Got 138610 / 151980 with accuracy 91.20\n",
            "Epoch [520/1000], Loss: 0.0000 And Got 141533 / 154960 with accuracy 91.34\n",
            "Epoch [530/1000], Loss: 0.0000 And Got 144456 / 157940 with accuracy 91.46\n",
            "Epoch [540/1000], Loss: 0.0000 And Got 147379 / 160920 with accuracy 91.59\n",
            "Epoch [550/1000], Loss: 0.0000 And Got 150303 / 163900 with accuracy 91.70\n",
            "Epoch [560/1000], Loss: 0.0000 And Got 153230 / 166880 with accuracy 91.82\n",
            "Epoch [570/1000], Loss: 0.0000 And Got 156148 / 169860 with accuracy 91.93\n",
            "Epoch [580/1000], Loss: 0.0000 And Got 159071 / 172840 with accuracy 92.03\n",
            "Epoch [590/1000], Loss: 0.0000 And Got 161996 / 175820 with accuracy 92.14\n",
            "Epoch [600/1000], Loss: 0.0000 And Got 164915 / 178800 with accuracy 92.23\n",
            "Epoch [610/1000], Loss: 0.0000 And Got 167840 / 181780 with accuracy 92.33\n",
            "Epoch [620/1000], Loss: 0.0000 And Got 170763 / 184760 with accuracy 92.42\n",
            "Epoch [630/1000], Loss: 0.0000 And Got 173684 / 187740 with accuracy 92.51\n",
            "Epoch [640/1000], Loss: 0.0000 And Got 176602 / 190720 with accuracy 92.60\n",
            "Epoch [650/1000], Loss: 0.0000 And Got 179525 / 193700 with accuracy 92.68\n",
            "Epoch [660/1000], Loss: 0.0000 And Got 182449 / 196680 with accuracy 92.76\n",
            "Epoch [670/1000], Loss: 0.0000 And Got 185378 / 199660 with accuracy 92.85\n",
            "Epoch [680/1000], Loss: 0.0000 And Got 188299 / 202640 with accuracy 92.92\n",
            "Epoch [690/1000], Loss: 0.0000 And Got 191221 / 205620 with accuracy 93.00\n",
            "Epoch [700/1000], Loss: 0.3608 And Got 194147 / 208600 with accuracy 93.07\n",
            "Epoch [710/1000], Loss: 0.0000 And Got 197072 / 211580 with accuracy 93.14\n",
            "Epoch [720/1000], Loss: 0.3624 And Got 199998 / 214560 with accuracy 93.21\n",
            "Epoch [730/1000], Loss: 0.0000 And Got 202924 / 217540 with accuracy 93.28\n",
            "Epoch [740/1000], Loss: 0.0000 And Got 205852 / 220520 with accuracy 93.35\n",
            "Epoch [750/1000], Loss: 0.0000 And Got 208779 / 223500 with accuracy 93.41\n",
            "Epoch [760/1000], Loss: 0.0000 And Got 211702 / 226480 with accuracy 93.47\n",
            "Epoch [770/1000], Loss: 0.0000 And Got 214628 / 229460 with accuracy 93.54\n",
            "Epoch [780/1000], Loss: 0.0000 And Got 217558 / 232440 with accuracy 93.60\n",
            "Epoch [790/1000], Loss: 0.3697 And Got 220478 / 235420 with accuracy 93.65\n",
            "Epoch [800/1000], Loss: 0.0000 And Got 223406 / 238400 with accuracy 93.71\n",
            "Epoch [810/1000], Loss: 0.0000 And Got 226326 / 241380 with accuracy 93.76\n",
            "Epoch [820/1000], Loss: 0.0000 And Got 229242 / 244360 with accuracy 93.81\n",
            "Epoch [830/1000], Loss: 0.0000 And Got 232166 / 247340 with accuracy 93.87\n",
            "Epoch [840/1000], Loss: 0.0000 And Got 235091 / 250320 with accuracy 93.92\n",
            "Epoch [850/1000], Loss: 0.0000 And Got 238014 / 253300 with accuracy 93.97\n",
            "Epoch [860/1000], Loss: 0.0000 And Got 240935 / 256280 with accuracy 94.01\n",
            "Epoch [870/1000], Loss: 0.0000 And Got 243860 / 259260 with accuracy 94.06\n",
            "Epoch [880/1000], Loss: 0.0000 And Got 246787 / 262240 with accuracy 94.11\n",
            "Epoch [890/1000], Loss: 0.0000 And Got 249713 / 265220 with accuracy 94.15\n",
            "Epoch [900/1000], Loss: 0.0000 And Got 252632 / 268200 with accuracy 94.20\n",
            "Epoch [910/1000], Loss: 0.0000 And Got 255556 / 271180 with accuracy 94.24\n",
            "Epoch [920/1000], Loss: 0.0000 And Got 258483 / 274160 with accuracy 94.28\n",
            "Epoch [930/1000], Loss: 0.0000 And Got 261413 / 277140 with accuracy 94.33\n",
            "Epoch [940/1000], Loss: 0.0000 And Got 264337 / 280120 with accuracy 94.37\n",
            "Epoch [950/1000], Loss: 0.0000 And Got 267265 / 283100 with accuracy 94.41\n",
            "Epoch [960/1000], Loss: 0.0000 And Got 270189 / 286080 with accuracy 94.45\n",
            "Epoch [970/1000], Loss: 0.0000 And Got 273115 / 289060 with accuracy 94.48\n",
            "Epoch [980/1000], Loss: 0.0000 And Got 276040 / 292040 with accuracy 94.52\n",
            "Epoch [990/1000], Loss: 0.0000 And Got 278963 / 295020 with accuracy 94.56\n",
            "Epoch [1000/1000], Loss: 0.0000 And Got 281892 / 298000 with accuracy 94.59\n",
            "final Accuracy-----> Got 281892 / 298000 with accuracy 94.59\n",
            "final loss: 0.0000\n",
            "training complete. file saved to DATA.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FILE = \"DATA.pth\"\n",
        "data = torch.load(FILE)\n",
        "\n",
        "input_size = data[\"input_size\"]\n",
        "hidden_size = data[\"hidden_size\"]\n",
        "output_size = data[\"output_size\"]\n",
        "all_the_words = data['all_words']\n",
        "tags = data['tags']\n",
        "model_state = data[\"model_state\"]\n",
        "\n",
        "model = NeuralNetModel(input_size, hidden_size, output_size).to(device)\n",
        "model.load_state_dict(model_state)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRuibvibva56",
        "outputId": "36c6056a-c659-4186-e915-cdbcf3b5f779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNetModel(\n",
              "  (linearlayer1): Linear(in_features=209, out_features=8, bias=True)\n",
              "  (bn1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (linearlayer2): Linear(in_features=8, out_features=8, bias=True)\n",
              "  (bn2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (linearlayer3): Linear(in_features=8, out_features=47, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def bargain(value):\n",
        "    tokens = nltk.word_tokenize(value)\n",
        "\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "    bot_name=\"FinBot\"\n",
        "\n",
        "    entities = nltk.chunk.ne_chunk(tagged)\n",
        "\n",
        "    numbers = {\"1\": 1, \"2\": 2, \"3\": 3, \"4\": 4, \"5\": 5, \"6\": 6, \"7\": 7, \"8\": 8, \"9\": 9, \"10\" : 10,\"11\": 11,\"12\": 12,\"13\": 13,\"14\": 14,\"15\": 15,\"16\": 16,\"17\": 17,\"18\": 18,\"19\": 19,\"20\": 20,\"21\": 21,\"22\": 22,\"23\": 23,\"24\": 24,\"25\": 25,\"26\": 26,\"27\": 27,\"28\": 28,\"29\":29,\"30\": 30}\n",
        "    OptionsToReply=[\"Sorry, this is of latest fashion, Can you raise the amount a little bit\",\"This is a very special thing, we can't give you at this much less cost\",\"Oh no sorry. Please raise a little bit\"]\n",
        "    for word, wordType in entities:\n",
        "        word = stemmer.stem(word)\n",
        "\n",
        "        if (wordType in ['CD'] and word in numbers):\n",
        "               if numbers[word] >20:\n",
        "                 print(\"FinBot:Yes agreed! Now,you can buy the ribbon at this price\")\n",
        "               elif numbers[word] <=20:\n",
        "                 print(f\"{bot_name}: {random.choice(OptionsToReply)}\")\n",
        "               else:\n",
        "                  break"
      ],
      "metadata": {
        "id": "dZGmIsalvv-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bargain('15')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRhzulSyvxpE",
        "outputId": "5f7b5ef9-911c-40ff-9de8-6a07fbce7cf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FinBot: Oh no sorry. Please raise a little bit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bargain('20')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24y1vmuov2qN",
        "outputId": "698ab904-462b-47fc-bf7a-b54e33b97e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FinBot: Oh no sorry. Please raise a little bit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bargain('25')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8s_TQDdv7B2",
        "outputId": "501997c8-b4cb-4794-df73-0844a2ea9142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FinBot:Yes agreed! Now,you can buy the ribbon at this price\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bot_name = \"FinBot\"\n",
        "\n",
        "name=input(\"Enter Your Name: \")\n",
        "print(\"FinBot:Hey, Let's chat! (type 'quit' to exit)Also when you start bargaining give digits\")\n",
        "while True:\n",
        "    sent=input(name+':')\n",
        "    if sent == \"quit\":\n",
        "        break\n",
        "\n",
        "    if sent in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\"]:\n",
        "       bargain(sent)\n",
        "    else:\n",
        "        sent = tokenize(sent)\n",
        "        X = BagOfWords(sent, all_the_words)\n",
        "        X = X.reshape(1, X.shape[0])\n",
        "        X = torch.from_numpy(X).to(device)\n",
        "\n",
        "        output = model(X)\n",
        "        _, predicted = torch.max(output, dim=1)\n",
        "\n",
        "        tag = tags[predicted.item()]\n",
        "\n",
        "        probs = torch.softmax(output, dim=1)\n",
        "        prob = probs[0][predicted.item()]\n",
        "        if prob.item() > 0.75:\n",
        "            for intent in intents['intents']:\n",
        "                if tag == intent[\"tag\"]:\n",
        "                    print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
        "        else:\n",
        "            print(f\"{bot_name}: I do not understand...\")"
      ],
      "metadata": {
        "id": "Z4si4gtyv9ZO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}